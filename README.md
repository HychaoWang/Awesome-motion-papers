### ✅ 修订版：人体动作生成代表性论文列表 (Human Motion Generation)

| 研究方向 | 论文标题 | 会议/年份 | 代表作者 | 备注 |
| :--- | :--- | :--- | :--- | :--- |
| **文本生成动作 (Text-to-Motion)** | **Generating Diverse and Natural 3D Human Motions from Text** | CVPR 2022 | Guo, C. et al. | 该领域必读经典，提出了两阶段生成范式 |
| | **T2M-GPT: Generating Human Motion from Textual Descriptions with GPT-based Motion VQ-VAE** | CVPR 2023 | Zhang, J. et al. | 将 VQ-VAE 与 GPT 结合的代表作 |
| | **MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model** | NeurIPS 2022 | Zhang, M. et al. | 第一篇将扩散模型(Diffusion)用于动作生成的顶会论文 |
| | **MDM: Human Motion Diffusion Model** | ICLR 2023 | Tevet, G. et al. | 另一篇极其重要的 Diffusion 动作生成工作 |
| **多人交互 (HHI)** | **InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions** | NeurIPS 2023 | Liang, H. et al. | 解决了多人交互中的长程依赖问题 |
| | **InterHuman: A Benchmark for Realistic Human-Human Interaction Modeling** | CVPR 2022 | Liang, H. et al. | 提供了重要的数据集和基准 |
| | **Action-Aware Interaction Generation** | CVPR 2022 | Zhao, L. et al. | 关注交互动作的语义一致性 |
| **人-物交互 (HOI)** | **GRAB: A Dataset for Human-Object Interaction with Fine-grained Grasping Labels** | ECCV 2020 | Taheri, O. et al. | 全身抓取与交互的经典数据集 |
| | **BEHAVE: A Multi-view RGB-D Dataset for Human-Object Interaction** | CVPR 2022 | Bhatnagar, B. et al. | 关注真实场景下的动态人-物交互 |
| | **HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction** | CVPR 2022 | Liu, Y. et al. | 大规模 4D 交互数据集 |
| **物理与强化学习 (Physics & RL)** | **ReinDiffuse: Crafting Physically Plausible Motions with Reinforced Diffusion Model** | WACV 2025 | Han, Y. et al. | 结合 RL 和 Diffusion 保证物理真实性 |
| | **DeepMimic: Example-Driven Training of 3D Character Controllers** | SIGGRAPH 2018 | Peng, X.B. et al. | 物理模拟动作的奠基之作 |
| | **PHC: Universal Humanoid Control** | CVPR 2024 | Luo, Z. et al. | 最新的物理仿真控制 SOTA |
| **动作编辑 (Editing)** | **Modi: Unconditional Motion Editing via Signal Decomposition** | CVPR 2023 | Raab, S. et al. | 专注于动作编辑的代表性工作 |
| | **GMD: Controllable Human Motion Generation via Graph-based Motion Diffusion** | IEEE T-PAMI 2024 | Karunrat, P. et al. | 图神经网络+扩散模型用于可控生成 |
| **实时性与效率 (Efficiency)** | **TM2T: Stochastic and Tokenized Modeling for the Reciprocal Generation of 3D Human Motion and Texts** | ECCV 2022 | Guo, C. et al. | 关注生成的紧凑性和效率 |
| | **MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Models** | ArXiv 2024 | Hu, Y. et al. | 利用 LCM 实现真正的实时生成 |
