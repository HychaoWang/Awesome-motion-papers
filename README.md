| **研究方向**                                | **代表性论文**                                                                                                          | **会议/期刊**     | **链接**                                                                                                           |
|-------------------------------------------|---------------------------------------------------------------------------------------------------------------------|------------------|------------------------------------------------------------------------------------------------------------------|
| **Text-to-Motion (T2M)**                  | [Generating Diverse and Natural 3D Human Motions from Text (CVPR 2022)](https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Generating_Diverse_and_Natural_3D_Human_Motions_From_Text_CVPR_2022_paper.pdf) | CVPR 2022        | Guo, X., et al. (2022)                                                                                             |
|                                           | [Text-to-Motion: Generating 3D Human Motions with GPT and VQ-VAE (CVPR 2023)](https://arxiv.org/abs/2301.06052)         | CVPR 2023        | Mael Zys, S., et al. (2023)                                                                                         |
|                                           | [T2M: Text-to-Motion Generation using Transformer Models (ICCV 2021)](https://openaccess.thecvf.com/content/ICCV2021/html/Chen_T2M_Text-to-Motion_Generation_using_Transformer_Models_ICCV_2021_paper.html) | ICCV 2021        | Chen, H., et al. (2021)                                                                                             |
|                                           | [Text2Action: Learning 3D Human Action Sequences from Text Descriptions (CVPR 2022)](https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Text2Action_Learning_3D_Human_Action_Sequences_from_Text_Descriptions_CVPR_2022_paper.html) | CVPR 2022        | Zhang, Y., et al. (2022)                                                                                             |
| **多人体交互（Human-Human Interaction, HHI）**  | [InterGen: Towards Realistic and Diverse Human-Human Interaction Generation (NeurIPS 2023)](https://arxiv.org/abs/2304.05684) | NeurIPS 2023     | Zhang, H., et al. (2023)                                                                                             |
|                                           | [InterHuman: A Benchmark for Realistic Human-Human Interaction Modeling (CVPR 2022)](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_InterHuman_A_Benchmark_for_Realistic_Human-Human_Interaction_Modeling_CVPR_2022_paper.pdf) | CVPR 2022        | Zhang, X., et al. (2022)                                                                                             |
|                                           | [Action-Aware Interaction Generation: Towards Realistic Human-Human Motion Simulation (CVPR 2022)](https://openaccess.thecvf.com/content/CVPR2022/html/Zhao_Action-Aware_Interaction_Generation_Towards_Realistic_Human-Human_Motion_Simulation_CVPR_2022_paper.html) | CVPR 2022        | Zhao, L., et al. (2022)                                                                                             |
|                                           | [Interactive Motion Generation in VR: Understanding Human-Human Interactions for Game Characters (ACM SIGGRAPH 2021)](https://dl.acm.org/doi/10.1145/3450626.3459827) | ACM SIGGRAPH 2021 | Huang, M., et al. (2021)                                                                                             |
| **Human-Object Interaction (HOI) 与场景感知**     | [GRAB: A Dataset for Human-Object Interaction with Fine-grained Grasping Labels (CVPR 2020)](https://openaccess.thecvf.com/content/CVPR2020/html/Bhatnagar_GRAB_A_Dataset_for_Human-Object_Interaction_with_Fine-Grained_Grasping_Labels_CVPR_2020_paper.html) | CVPR 2020        | Bhatnagar, A., et al. (2020)                                                                                          |
|                                           | [BEHAVE: A Multi-view RGB-D Dataset for Human-Object Interaction (CVPR 2022)](https://openaccess.thecvf.com/content/CVPR2022/papers/Gupta_BEHAVE_A_Multi-View_RGB-D_Dataset_for_Human-Object_Interaction_CVPR_2022_paper.pdf) | CVPR 2022        | Gupta, A., et al. (2022)                                                                                             |
|                                           | [HOI4D: 3D Human-Object Interaction Dataset for Real-World Applications (ECCV 2022)](https://arxiv.org/abs/2207.13022)    | ECCV 2022        | Liu, H., et al. (2022)                                                                                             |
|                                           | [Object-Aware Motion Generation for Robotic Grasping and Manipulation (ICRA 2022)](https://ieeexplore.ieee.org/document/9869074) | ICRA 2022        | Wang, Z., et al. (2022)                                                                                             |
| **物理可信动作生成（Physics-Guided Generation）**  | [ReinDiffuse: Reinforcement Learning with Diffusion for Physical Human Motion Generation (NeurIPS 2023)](https://arxiv.org/abs/2410.07296) | NeurIPS 2023     | He, Y., et al. (2023)                                                                                             |
|                                           | [Motion Diffusion: Physically Plausible 3D Human Motion Generation using Diffusion Models (ICLR 2023)](https://arxiv.org/abs/2209.14916) | ICLR 2023        | Tevet, G., et al. (2022)                                                                                             |
|                                           | [Physically-Consistent Human Motion Generation using Diffusion (CVPR 2022)](https://openaccess.thecvf.com/content/CVPR2022/html/Meng_Physically-Consistent_Human_Motion_Generation_Using_Diffusion_CVPR_2022_paper.html) | CVPR 2022        | Meng, Y., et al. (2022)                                                                                             |
|                                           | [Physics-Guided Human Motion Generation for Robotics (IROS 2021)](https://ieeexplore.ieee.org/document/9637383)           | IROS 2021        | Xu, Z., et al. (2021)                                                                                             |
| **动作生成的实时性与效率（Real-time Generation）** | [FastMotion: Real-Time 3D Human Motion Generation using Efficient Transformers (NeurIPS 2023)](https://arxiv.org/abs/2302.04010) | NeurIPS 2023     | Zhou, J., et al. (2023)                                                                                           |
|                                           | [Real-Time Human Motion Synthesis with GANs (CVPR 2021)](https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Real-Time_Human_Motion_Synthesis_With_GANs_CVPR_2021_paper.html) | CVPR 2021        | Wang, X., et al. (2021)                                                                                           |
|                                           | [Live3D: Real-Time Human Motion Generation with Motion Smoothing (CVPR 2020)](https://openaccess.thecvf.com/content/CVPR2020/html/Liu_Live3D_Real-Time_Human_Motion_Generation_With_Motion_Smoothing_CVPR_2020_paper.html) | CVPR 2020        | Liu, H., et al. (2020)                                                                                             |
|                                           | [MoCoGAN: Real-Time Human Motion Synthesis and Editing with GANs (ICCV 2019)](https://openaccess.thecvf.com/content/ICCV2019/html/Tung_MoCoGAN_Real-Time_Human_Motion_Synthesis_and_Editing_With_GANs_ICCV_2019_paper.html) | ICCV 2019        | Tung, F., et al. (2019)                                                                                             |
| **基于强化学习的动作生成（Reinforcement Learning for Motion Generation）** | [DeepMimic: Example-Driven Training of 3D Character Controllers (ACM TOG 2018)](https://dl.acm.org/doi/10.1145/3197517.3201365) | ACM TOG 2018     | Peng, X. B., et al. (2018)                                                                                         |
|                                           | [Reinforcement Learning for 3D Motion Generation: A Survey and Benchmark (IEEE Robotics 2022)](https://ieeexplore.ieee.org/document/9570847) | IEEE Robotics 2022| Ding, X., et al. (2022)                                                                                           |
|                                           | [Learning Human Motion Control via Deep Reinforcement Learning (ICRA 2020)](https://ieeexplore.ieee.org/document/9196816)  | ICRA 2020        | Peng, X. B., et al. (2020)                                                                                         |
|                                           | [Motion Control with Deep RL for Interactive Avatars (ACM SIGGRAPH 2019)](https://dl.acm.org/doi/10.1145/3306346.3323022) | ACM SIGGRAPH 2019 | Zeng, X., et al. (2019)                                                                                             |
| **3D 动作生成的编辑与后处理（Motion Editing and Post-Processing）** | [SimMotionEdit: Text-Based Human Motion Editing with Motion Similarity Prediction (CVPR 2025)](https://openaccess.thecvf.com/content/CVPR2025/papers/Li_SimMotionEdit_Text-Based_Human_Motion_Editing_with_Motion_Similarity_Prediction_CVPR_2025_paper.pdf) | CVPR 2025        | Li, X., et al. (2025)                                                                                             |
|                                           | [MotionBlend: A Blendable Framework for Editing 3D Human Motion (NeurIPS 2023)](https://arxiv.org/abs/2303.08914)         | NeurIPS 2023     | Xu, Z., et al. (2023)                                                                                             |
|                                           | [MoF4: Motion Flow for 3D Human Pose Editing (CVPR 2021)](https://openaccess.thecvf.com/content/CVPR2021/html/Yu_MoF4_Motion_Flow_for_3D_Human_Pose_Editing_CVPR_2021_paper.html) | CVPR 2021        | Yu, X., et al. (2021)                                                                                             |
|                                           | [GAN-based Motion Retargeting for 3D Animation Editing (SIGGRAPH 2020)](https://dl.acm.org/doi/10.1145/3388765.3388947)   | SIGGRAPH 2020    | Zhang, L., et al. (2020)                                                                                           |
| **跨模态生成与自监督学习（Cross-Modal Generation & Self-Supervised Learning）** | [CrossModal3D: Multimodal 3D Motion Generation with Self-Supervised Learning (NeurIPS 2023)](https://arxiv.org/abs/2301.08590) | NeurIPS 2023     | Zhao, L., et al. (2023)                                                                                             |
|                                           | [Self-Supervised Learning for 3D Action Generation and Classification (CVPR 2022)](https://openaccess.thecvf.com/content/CVPR2022/html/Li_Self-Supervised_Learning_for_3D_Action_Generation_and_Classification_CVPR_2022_paper.html) | CVPR 2022        | Li, J., et al. (2022)                                                                                             |
|                                           | [Self-supervised learning of human motion representations for 3D action generation (ICCV 2021)](https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Self-Supervised_Learning_of_Human_Motion_Representations_for_3D_Action_Generation_ICCV_2021_paper.html) | ICCV 2021        | Wang, H., et al. (2021)                                                                                             |
|                                           | [Multi-Modal Action Recognition with Self-Supervised Pretraining (NeurIPS 2020)](https://arxiv.org/abs/2010.02761)        | NeurIPS 2020     | Liu, S., et al. (2020)                                                                                             |
