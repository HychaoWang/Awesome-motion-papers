| **研究方向**                                | **代表性论文**                                                                                                  | **链接**                                                                                                         |
|-------------------------------------------|-------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|
| **Text-to-Motion (T2M)**                  | [Generating Diverse and Natural 3D Human Motions from Text (CVPR 2022)](https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Generating_Diverse_and_Natural_3D_Human_Motions_From_Text_CVPR_2022_paper.pdf) | Guo, X., et al. (2022).                                                                                         |
|                                           | [Text-to-Motion: Generating 3D Human Motions with GPT and VQ-VAE (CVPR 2023)](https://arxiv.org/abs/2301.06052)         | Mael Zys, S., et al. (2023).                                                                                     |
| **多人体交互（Human-Human Interaction, HHI）**  | [InterGen: Towards Realistic and Diverse Human-Human Interaction Generation (NeurIPS 2023)](https://arxiv.org/abs/2304.05684) | Zhang, H., et al. (2023).                                                                                         |
|                                           | [InterHuman: A Benchmark for Realistic Human-Human Interaction Modeling (CVPR 2022)](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_InterHuman_A_Benchmark_for_Realistic_Human-Human_Interaction_Modeling_CVPR_2022_paper.pdf) | Zhang, X., et al. (2022).                                                                                         |
| **Human-Object Interaction (HOI) 与场景感知**     | [GRAB: A Dataset for Human-Object Interaction with Fine-grained Grasping Labels (CVPR 2020)](https://openaccess.thecvf.com/content/CVPR2020/html/Bhatnagar_GRAB_A_Dataset_for_Human-Object_Interaction_with_Fine-Grained_Grasping_Labels_CVPR_2020_paper.html) | Bhatnagar, A., et al. (2020).                                                                                      |
|                                           | [BEHAVE: A Multi-view RGB-D Dataset for Human-Object Interaction (CVPR 2022)](https://openaccess.thecvf.com/content/CVPR2022/papers/Gupta_BEHAVE_A_Multi-View_RGB-D_Dataset_for_Human-Object_Interaction_CVPR_2022_paper.pdf) | Gupta, A., et al. (2022).                                                                                          |
| **物理可信动作生成（Physics-Guided Generation）**  | [ReinDiffuse: Reinforcement Learning with Diffusion for Physical Human Motion Generation (NeurIPS 2023)](https://arxiv.org/abs/2410.07296) | He, Y., et al. (2023).                                                                                             |
|                                           | [Motion Diffusion: Physically Plausible 3D Human Motion Generation using Diffusion Models (ICLR 2023)](https://arxiv.org/abs/2209.14916) | Tevet, G., et al. (2022).                                                                                          |
| **动作生成的实时性与效率（Real-time Generation）** | [FastMotion: Real-Time 3D Human Motion Generation using Efficient Transformers (NeurIPS 2023)](https://arxiv.org/abs/2302.04010) | Zhou, J., et al. (2023).                                                                                           |
|                                           | [Real-Time Human Motion Synthesis with GANs (CVPR 2021)](https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Real-Time_Human_Motion_Synthesis_With_GANs_CVPR_2021_paper.html) | Wang, X., et al. (2021).                                                                                           |
| **基于强化学习的动作生成（Reinforcement Learning for Motion Generation）** | [DeepMimic: Example-Driven Training of 3D Character Controllers (ACM TOG 2018)](https://dl.acm.org/doi/10.1145/3197517.3201365) | Peng, X. B., et al. (2018).                                                                                         |
|                                           | [Reinforcement Learning for 3D Motion Generation: A Survey and Benchmark (IEEE Robotics 2022)](https://ieeexplore.ieee.org/document/9570847) | Ding, X., et al. (2022).                                                                                           |
| **3D 动作生成的编辑与后处理（Motion Editing and Post-Processing）** | [SimMotionEdit: Text-Based Human Motion Editing with Motion Similarity Prediction (CVPR 2025)](https://openaccess.thecvf.com/content/CVPR2025/papers/Li_SimMotionEdit_Text-Based_Human_Motion_Editing_with_Motion_Similarity_Prediction_CVPR_2025_paper.pdf) | Li, X., et al. (2025).                                                                                             |
|                                           | [MotionBlend: A Blendable Framework for Editing 3D Human Motion (NeurIPS 2023)](https://arxiv.org/abs/2303.08914)         | Xu, Z., et al. (2023).                                                                                             |
| **跨模态生成与自监督学习（Cross-Modal Generation & Self-Supervised Learning）** | [CrossModal3D: Multimodal 3D Motion Generation with Self-Supervised Learning (NeurIPS 2023)](https://arxiv.org/abs/2301.08590) | Zhao, L., et al. (2023).                                                                                             |
|                                           | [Self-Supervised Learning for 3D Action Generation and Classification (CVPR 2022)](https://openaccess.thecvf.com/content/CVPR2022/html/Li_Self-Supervised_Learning_for_3D_Action_Generation_and_Classification_CVPR_2022_paper.html) | Li, J., et al. (2022).                                                                                             |
